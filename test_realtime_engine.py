"""
å®æ—¶å†³ç­–å¼•æ“æµ‹è¯•è„šæœ¬
"""
import sys
sys.path.append('src')

from innovation.realtime_engine import RealtimeDecisionEngine, DecisionResult
import time
import numpy as np

def test_realtime_performance():
    """æµ‹è¯•å®æ—¶æ€§èƒ½"""
    print("ğŸš€ å®æ—¶å†³ç­–å¼•æ“æ€§èƒ½æµ‹è¯•")
    print("="*50)
    
    # åˆ›å»ºå¼•æ“
    engine = RealtimeDecisionEngine()
    
    # è®¾ç½®å›è°ƒå‡½æ•°
    high_priority_decisions = []
    
    def priority_callback(decision: DecisionResult):
        if decision.confidence > 0.85:
            high_priority_decisions.append(decision)
            print(f"âš¡ é«˜ä¼˜å…ˆçº§å†³ç­–: {decision.action} | ç½®ä¿¡åº¦: {decision.confidence:.2f}")
    
    engine.add_decision_callback(priority_callback)
    
    # è¿è¡Œæµ‹è¯•
    print("å¯åŠ¨å®æ—¶æ•°æ®æµå¤„ç†...")
    start_time = time.time()
    
    # è¿è¡Œ30ç§’çš„å®æ—¶å¤„ç†
    engine.process_live_data(duration_seconds=30)
    
    total_time = time.time() - start_time
    
    # è·å–æ€§èƒ½æŠ¥å‘Š
    report = engine.get_performance_report()
    
    print(f"\nğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ:")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"æ€»è¿è¡Œæ—¶é—´: {total_time:.1f} ç§’")
    print(f"å¤„ç†æ•°æ®é‡: {report['metrics']['processed_count']} æ¡")
    print(f"å†³ç­–æ•°é‡: {report['metrics']['decision_count']} ä¸ª")
    print(f"å¹³å‡å»¶è¿Ÿ: {report['avg_latency_ms']:.1f} ms")
    print(f"ååé‡: {report['metrics']['processed_count']/total_time:.1f} æ¡/ç§’")
    print(f"é”™è¯¯ç‡: {report['error_rate']:.2%}")
    print(f"é«˜ä¼˜å…ˆçº§å†³ç­–: {len(high_priority_decisions)} ä¸ª")
    print(f"æ•°æ®åº“è®°å½•: {report['database_records']} æ¡")
    
    # æµ‹è¯•è¾¹ç¼˜è®¡ç®—éƒ¨ç½²
    print(f"\nğŸ”§ è¾¹ç¼˜è®¡ç®—éƒ¨ç½²æµ‹è¯•:")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    edge_configs = engine.edge_computing()
    
    for platform, config in edge_configs.items():
        print(f"ğŸ“± {platform}:")
        print(f"   æ¨ç†å»¶è¿Ÿ: {config.get('inference_latency', 'N/A')}")
        print(f"   å†…å­˜éœ€æ±‚: {config.get('memory_requirement', 'N/A')}")
        print(f"   æ¨¡å‹å¤§å°: {config.get('model_size', 'N/A')} bytes")
    
    # éªŒè¯å®æ—¶æ€§èƒ½æŒ‡æ ‡
    print(f"\nâœ… å®æ—¶æ€§èƒ½éªŒè¯:")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    latency_ok = report['avg_latency_ms'] < 100  # å»¶è¿Ÿå°äº100ms
    throughput_ok = report['metrics']['processed_count']/total_time > 1  # ååé‡å¤§äº1æ¡/ç§’
    error_rate_ok = report['error_rate'] < 0.01  # é”™è¯¯ç‡å°äº1%
    
    print(f"å»¶è¿Ÿè¦æ±‚ (<100ms): {'âœ…' if latency_ok else 'âŒ'} {report['avg_latency_ms']:.1f}ms")
    print(f"ååé‡è¦æ±‚ (>1æ¡/ç§’): {'âœ…' if throughput_ok else 'âŒ'} {report['metrics']['processed_count']/total_time:.1f}æ¡/ç§’")
    print(f"é”™è¯¯ç‡è¦æ±‚ (<1%): {'âœ…' if error_rate_ok else 'âŒ'} {report['error_rate']:.2%}")
    
    # æ¸…ç†èµ„æº
    engine.stop()
    
    # è¿”å›æµ‹è¯•ç»“æœ
    return {
        'latency_ms': report['avg_latency_ms'],
        'throughput_per_sec': report['metrics']['processed_count']/total_time,
        'error_rate': report['error_rate'],
        'total_decisions': report['metrics']['decision_count'],
        'high_priority_decisions': len(high_priority_decisions),
        'edge_deployment_ready': len(edge_configs) > 0
    }

def test_scalability():
    """æµ‹è¯•å¯æ‰©å±•æ€§"""
    print(f"\nğŸ”„ å¯æ‰©å±•æ€§æµ‹è¯•")
    print("="*50)
    
    test_durations = [10, 20, 30]  # ä¸åŒçš„æµ‹è¯•æ—¶é•¿
    results = []
    
    for duration in test_durations:
        print(f"\næµ‹è¯• {duration} ç§’è¿è¡Œ...")
        
        engine = RealtimeDecisionEngine()
        start_time = time.time()
        
        engine.process_live_data(duration_seconds=duration)
        
        actual_time = time.time() - start_time
        report = engine.get_performance_report()
        
        result = {
            'duration': duration,
            'actual_time': actual_time,
            'processed_count': report['metrics']['processed_count'],
            'avg_latency': report['avg_latency_ms'],
            'throughput': report['metrics']['processed_count'] / actual_time
        }
        
        results.append(result)
        engine.stop()
        
        print(f"  å¤„ç†: {result['processed_count']} æ¡")
        print(f"  å»¶è¿Ÿ: {result['avg_latency']:.1f} ms")
        print(f"  åå: {result['throughput']:.1f} æ¡/ç§’")
    
    # åˆ†ææ‰©å±•æ€§
    print(f"\nğŸ“ˆ æ‰©å±•æ€§åˆ†æ:")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    for i, result in enumerate(results):
        if i > 0:
            prev_result = results[i-1]
            duration_ratio = result['duration'] / prev_result['duration']
            throughput_ratio = result['throughput'] / prev_result['throughput']
            scalability = throughput_ratio / duration_ratio
            
            print(f"æ—¶é•¿ {prev_result['duration']}s â†’ {result['duration']}s:")
            print(f"  æ—¶é•¿å€æ•°: {duration_ratio:.1f}x")
            print(f"  ååå€æ•°: {throughput_ratio:.1f}x")
            print(f"  æ‰©å±•æ€§æŒ‡æ ‡: {scalability:.2f} {'âœ…' if scalability > 0.8 else 'âŒ'}")
    
    return results

def benchmark_decision_quality():
    """å†³ç­–è´¨é‡åŸºå‡†æµ‹è¯•"""
    print(f"\nğŸ¯ å†³ç­–è´¨é‡åŸºå‡†æµ‹è¯•")
    print("="*50)
    
    engine = RealtimeDecisionEngine()
    
    # æ”¶é›†å†³ç­–æ•°æ®
    decisions = []
    
    def collect_decisions(decision: DecisionResult):
        decisions.append(decision)
    
    engine.add_decision_callback(collect_decisions)
    
    # è¿è¡Œå†³ç­–æ”¶é›†
    engine.process_live_data(duration_seconds=20)
    
    # åˆ†æå†³ç­–è´¨é‡
    if decisions:
        confidences = [d.confidence for d in decisions]
        benefits = [d.expected_benefit for d in decisions]
        
        avg_confidence = np.mean(confidences)
        avg_benefit = np.mean(benefits)
        high_confidence_ratio = sum(1 for c in confidences if c > 0.8) / len(confidences)
        
        decision_types = {}
        for d in decisions:
            decision_types[d.decision_type] = decision_types.get(d.decision_type, 0) + 1
        
        print(f"å†³ç­–æ•°é‡: {len(decisions)}")
        print(f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
        print(f"å¹³å‡é¢„æœŸæ”¶ç›Š: {avg_benefit:.1f}")
        print(f"é«˜ç½®ä¿¡åº¦æ¯”ä¾‹: {high_confidence_ratio:.1%}")
        print(f"å†³ç­–ç±»å‹åˆ†å¸ƒ:")
        for dtype, count in decision_types.items():
            print(f"  {dtype}: {count} ({count/len(decisions):.1%})")
    
    engine.stop()
    
    return {
        'total_decisions': len(decisions),
        'avg_confidence': avg_confidence if decisions else 0,
        'avg_benefit': avg_benefit if decisions else 0,
        'high_confidence_ratio': high_confidence_ratio if decisions else 0
    }

def generate_test_report():
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print("ğŸ”¬ å®æ—¶å†³ç­–å¼•æ“ç»¼åˆæµ‹è¯•æŠ¥å‘Š")
    print("="*60)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    print("æ­£åœ¨è¿è¡Œæ€§èƒ½æµ‹è¯•...")
    perf_results = test_realtime_performance()
    
    print("æ­£åœ¨è¿è¡Œå¯æ‰©å±•æ€§æµ‹è¯•...")
    scale_results = test_scalability()
    
    print("æ­£åœ¨è¿è¡Œå†³ç­–è´¨é‡æµ‹è¯•...")
    quality_results = benchmark_decision_quality()
    
    # ç”ŸæˆæŠ¥å‘Š
    print(f"\nğŸ“‹ ç»¼åˆæµ‹è¯•æŠ¥å‘Š")
    print("="*60)
    
    print(f"\nğŸš€ æ€§èƒ½æŒ‡æ ‡:")
    print(f"  å¹³å‡å»¶è¿Ÿ: {perf_results['latency_ms']:.1f} ms")
    print(f"  æ•°æ®åå: {perf_results['throughput_per_sec']:.1f} æ¡/ç§’")
    print(f"  é”™è¯¯ç‡: {perf_results['error_rate']:.2%}")
    print(f"  æ€»å†³ç­–æ•°: {perf_results['total_decisions']}")
    
    print(f"\nğŸ“ˆ æ‰©å±•æ€§æŒ‡æ ‡:")
    if len(scale_results) >= 2:
        last_result = scale_results[-1]
        first_result = scale_results[0]
        scale_efficiency = (last_result['throughput'] / first_result['throughput']) / (last_result['duration'] / first_result['duration'])
        print(f"  æ‰©å±•æ•ˆç‡: {scale_efficiency:.2f}")
        print(f"  æœ€å¤§åå: {max(r['throughput'] for r in scale_results):.1f} æ¡/ç§’")
    
    print(f"\nğŸ¯ å†³ç­–è´¨é‡:")
    print(f"  å¹³å‡ç½®ä¿¡åº¦: {quality_results['avg_confidence']:.3f}")
    print(f"  å¹³å‡é¢„æœŸæ”¶ç›Š: {quality_results['avg_benefit']:.1f}")
    print(f"  é«˜ç½®ä¿¡åº¦æ¯”ä¾‹: {quality_results['high_confidence_ratio']:.1%}")
    
    print(f"\nğŸ”§ è¾¹ç¼˜éƒ¨ç½²:")
    print(f"  éƒ¨ç½²å°±ç»ª: {'âœ…' if perf_results['edge_deployment_ready'] else 'âŒ'}")
    
    print(f"\nğŸ† æ€»ä½“è¯„ä¼°:")
    
    # è®¡ç®—ç»¼åˆè¯„åˆ†
    performance_score = min(100, (100 - perf_results['latency_ms']) + perf_results['throughput_per_sec'] * 10)
    quality_score = quality_results['avg_confidence'] * 100
    reliability_score = (1 - perf_results['error_rate']) * 100
    
    overall_score = (performance_score + quality_score + reliability_score) / 3
    
    print(f"  æ€§èƒ½è¯„åˆ†: {performance_score:.1f}/100")
    print(f"  è´¨é‡è¯„åˆ†: {quality_score:.1f}/100")
    print(f"  å¯é æ€§è¯„åˆ†: {reliability_score:.1f}/100")
    print(f"  ç»¼åˆè¯„åˆ†: {overall_score:.1f}/100")
    
    if overall_score >= 90:
        print(f"  ç­‰çº§: ğŸ¥‡ ä¼˜ç§€")
    elif overall_score >= 80:
        print(f"  ç­‰çº§: ğŸ¥ˆ è‰¯å¥½")
    elif overall_score >= 70:
        print(f"  ç­‰çº§: ğŸ¥‰ åˆæ ¼")
    else:
        print(f"  ç­‰çº§: âŒ éœ€è¦æ”¹è¿›")
    
    print(f"\nğŸ’¡ å»ºè®®:")
    if perf_results['latency_ms'] > 50:
        print(f"  â€¢ ä¼˜åŒ–ç®—æ³•ä»¥é™ä½å»¶è¿Ÿ")
    if perf_results['error_rate'] > 0.005:
        print(f"  â€¢ å¢å¼ºé”™è¯¯å¤„ç†æœºåˆ¶")
    if quality_results['high_confidence_ratio'] < 0.7:
        print(f"  â€¢ æ”¹è¿›å†³ç­–æ¨¡å‹å‡†ç¡®æ€§")
    
    print(f"\nâœ… å®æ—¶å†³ç­–å¼•æ“æµ‹è¯•å®Œæˆï¼")
    
    return {
        'performance': perf_results,
        'scalability': scale_results,
        'quality': quality_results,
        'overall_score': overall_score
    }

if __name__ == '__main__':
    # è¿è¡Œç»¼åˆæµ‹è¯•
    test_results = generate_test_report()
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    import json
    with open('output/realtime_engine_test_report.json', 'w', encoding='utf-8') as f:
        json.dump(test_results, f, indent=2, ensure_ascii=False) 