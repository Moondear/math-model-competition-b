"""
å®æ—¶å†³ç­–å¼•æ“ç®€åŒ–æ¼”ç¤º
"""
import time
import threading
import queue
import json
import numpy as np
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ProductionData:
    """ç”Ÿäº§æ•°æ®ç»“æ„"""
    timestamp: float
    batch_id: str
    defect_rate: float
    throughput: int
    quality_score: float
    machine_status: str
    temperature: float
    pressure: float

@dataclass
class DecisionResult:
    """å†³ç­–ç»“æœç»“æ„"""
    timestamp: float
    decision_type: str
    action: str
    confidence: float
    expected_benefit: float
    risk_level: str

class SimpleStreamProcessor:
    """ç®€åŒ–æµå¤„ç†å™¨"""
    
    def __init__(self, buffer_size: int = 1000):
        self.buffer = queue.Queue(maxsize=buffer_size)
        self.is_running = False
        
    def start_simulation(self):
        """å¯åŠ¨æ•°æ®æµæ¨¡æ‹Ÿ"""
        self.is_running = True
        
        def generate_data():
            batch_counter = 0
            while self.is_running:
                data = ProductionData(
                    timestamp=time.time(),
                    batch_id=f"BATCH_{batch_counter:06d}",
                    defect_rate=np.random.uniform(0.02, 0.15),
                    throughput=np.random.randint(80, 120),
                    quality_score=np.random.uniform(0.85, 0.98),
                    machine_status=np.random.choice(['normal', 'warning', 'maintenance']),
                    temperature=np.random.uniform(20, 80),
                    pressure=np.random.uniform(1.0, 5.0)
                )
                
                try:
                    self.buffer.put_nowait(data)
                    batch_counter += 1
                    time.sleep(0.5)  # æ¯0.5ç§’ç”Ÿæˆä¸€æ¡æ•°æ®
                except queue.Full:
                    logger.warning("æ•°æ®ç¼“å†²åŒºå·²æ»¡ï¼Œä¸¢å¼ƒæ•°æ®")
                    
        thread = threading.Thread(target=generate_data, daemon=True)
        thread.start()
        logger.info("æµæ•°æ®æ¨¡æ‹Ÿå·²å¯åŠ¨")
        
    def fetch(self, timeout: float = 1.0) -> Optional[ProductionData]:
        """è·å–æµæ•°æ®"""
        try:
            return self.buffer.get(timeout=timeout)
        except queue.Empty:
            return None
            
    def stop(self):
        """åœæ­¢æµå¤„ç†"""
        self.is_running = False

class SimpleIncrementalSolver:
    """ç®€åŒ–å¢é‡æ±‚è§£å™¨"""
    
    def __init__(self):
        self.current_solution = {}
        self.last_update = time.time()
        self.solution_history = []
        
    def solve_incremental(self, new_data: ProductionData) -> DecisionResult:
        """å¢é‡æ±‚è§£"""
        # åˆ†ææ–°æ•°æ®çš„å½±å“
        impact_score = self._analyze_data_impact(new_data)
        
        # æ ¹æ®å½±å“ç¨‹åº¦å†³å®šæ±‚è§£ç­–ç•¥
        if impact_score > 0.7:
            decision = self._full_resolve(new_data)
        elif impact_score > 0.3:
            decision = self._partial_resolve(new_data)
        else:
            decision = self._parameter_adjustment(new_data)
            
        self._update_solution_state(decision)
        return decision
        
    def _analyze_data_impact(self, data: ProductionData) -> float:
        """åˆ†ææ•°æ®å½±å“ç¨‹åº¦"""
        impact = 0.0
        
        if hasattr(self, '_last_defect_rate'):
            defect_change = abs(data.defect_rate - self._last_defect_rate)
            impact += defect_change * 2.0
            
        if hasattr(self, '_last_quality'):
            quality_change = abs(data.quality_score - self._last_quality)
            impact += quality_change * 1.5
            
        if data.machine_status == 'maintenance':
            impact += 0.8
        elif data.machine_status == 'warning':
            impact += 0.4
            
        self._last_defect_rate = data.defect_rate
        self._last_quality = data.quality_score
        
        return min(impact, 1.0)
        
    def _full_resolve(self, data: ProductionData) -> DecisionResult:
        """å®Œå…¨é‡æ–°æ±‚è§£"""
        time.sleep(0.05)  # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
        
        if data.defect_rate > 0.1:
            action = "åŠ å¼ºæ£€æµ‹"
            confidence = 0.9
            benefit = 1000 * (0.15 - data.defect_rate)
            risk = "low"
        elif data.quality_score < 0.9:
            action = "è°ƒæ•´å·¥è‰º"
            confidence = 0.85
            benefit = 800 * (0.95 - data.quality_score)
            risk = "medium"
        else:
            action = "ç»´æŒå½“å‰"
            confidence = 0.95
            benefit = 100
            risk = "low"
            
        return DecisionResult(
            timestamp=time.time(),
            decision_type="full_optimization",
            action=action,
            confidence=confidence,
            expected_benefit=benefit,
            risk_level=risk
        )
        
    def _partial_resolve(self, data: ProductionData) -> DecisionResult:
        """å±€éƒ¨ä¼˜åŒ–"""
        time.sleep(0.02)
        
        return DecisionResult(
            timestamp=time.time(),
            decision_type="partial_optimization",
            action="å‚æ•°å¾®è°ƒ",
            confidence=0.8,
            expected_benefit=200,
            risk_level="low"
        )
        
    def _parameter_adjustment(self, data: ProductionData) -> DecisionResult:
        """å‚æ•°å¾®è°ƒ"""
        return DecisionResult(
            timestamp=time.time(),
            decision_type="parameter_adjustment",
            action="ç›‘æ§ç»§ç»­",
            confidence=0.7,
            expected_benefit=50,
            risk_level="minimal"
        )
        
    def _update_solution_state(self, decision: DecisionResult):
        """æ›´æ–°è§£çŠ¶æ€"""
        self.current_solution['last_decision'] = asdict(decision)
        self.last_update = time.time()
        self.solution_history.append(decision)
        
        if len(self.solution_history) > 1000:
            self.solution_history = self.solution_history[-1000:]

class SimpleRealtimeEngine:
    """ç®€åŒ–å®æ—¶å†³ç­–å¼•æ“"""
    
    def __init__(self):
        self.stream_processor = SimpleStreamProcessor()
        self.solver = SimpleIncrementalSolver()
        self.is_running = False
        self.decision_callbacks = []
        self.metrics = {
            'processed_count': 0,
            'decision_count': 0,
            'avg_latency': 0.0,
            'error_count': 0,
            'start_time': time.time()
        }
        
    def add_decision_callback(self, callback):
        """æ·»åŠ å†³ç­–å›è°ƒå‡½æ•°"""
        self.decision_callbacks.append(callback)
        
    def process_live_data(self, duration_seconds: int = 60):
        """å®æ—¶å¤„ç†ç”Ÿäº§æ•°æ®æµ"""
        logger.info(f"å¯åŠ¨å®æ—¶å†³ç­–å¼•æ“ï¼Œè¿è¡Œ{duration_seconds}ç§’...")
        
        self.is_running = True
        self.stream_processor.start_simulation()
        self.metrics['start_time'] = time.time()
        
        start_time = time.time()
        last_update = start_time
        
        try:
            while self.is_running and (time.time() - start_time) < duration_seconds:
                new_data = self.stream_processor.fetch(timeout=1.0)
                
                if new_data is None:
                    continue
                    
                try:
                    process_start = time.time()
                    decision = self.solver.solve_incremental(new_data)
                    latency = time.time() - process_start
                    
                    self._update_metrics(latency)
                    
                    for callback in self.decision_callbacks:
                        try:
                            callback(decision)
                        except Exception as e:
                            logger.error(f"å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
                    
                    if time.time() - last_update >= 5.0:
                        self._log_status(new_data, decision)
                        last_update = time.time()
                        
                except Exception as e:
                    logger.error(f"å¤„ç†æ•°æ®æ—¶å‡ºé”™: {e}")
                    self.metrics['error_count'] += 1
                    
        finally:
            self.stream_processor.stop()
            self.is_running = False
            logger.info("å®æ—¶å†³ç­–å¼•æ“å·²åœæ­¢")
            
    def _update_metrics(self, latency: float):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        self.metrics['processed_count'] += 1
        self.metrics['decision_count'] += 1
        
        count = self.metrics['processed_count']
        current_avg = self.metrics['avg_latency']
        self.metrics['avg_latency'] = (current_avg * (count - 1) + latency) / count
        
    def _log_status(self, data: ProductionData, decision: DecisionResult):
        """è¾“å‡ºçŠ¶æ€ä¿¡æ¯"""
        logger.info(f"å®æ—¶å†³ç­–çŠ¶æ€:")
        logger.info(f"  æ‰¹æ¬¡: {data.batch_id}")
        logger.info(f"  æ¬¡å“ç‡: {data.defect_rate:.3f}")
        logger.info(f"  è´¨é‡åˆ†æ•°: {data.quality_score:.3f}")
        logger.info(f"  å†³ç­–: {decision.action}")
        logger.info(f"  ç½®ä¿¡åº¦: {decision.confidence:.3f}")
        logger.info(f"  é¢„æœŸæ”¶ç›Š: {decision.expected_benefit:.1f}")
        logger.info(f"  å¤„ç†å»¶è¿Ÿ: {self.metrics['avg_latency']*1000:.1f}ms")
        
    def edge_computing_demo(self) -> Dict:
        """è¾¹ç¼˜è®¡ç®—éƒ¨ç½²æ¼”ç¤º"""
        logger.info("æ¨¡æ‹Ÿè¾¹ç¼˜è®¡ç®—éƒ¨ç½²...")
        
        # ç”Ÿæˆæ¨¡æ‹Ÿé…ç½®
        configs = {
            'raspberry_pi': {
                'inference_latency': '20ms',
                'memory_requirement': '128MB',
                'model_size': '2.5MB',
                'power_consumption': '5W'
            },
            'edge_tpu': {
                'inference_latency': '5ms',
                'memory_requirement': '64MB',
                'model_size': '1.8MB',
                'power_consumption': '2W'
            },
            'nvidia_jetson': {
                'inference_latency': '10ms',
                'memory_requirement': '256MB',
                'model_size': '3.2MB',
                'power_consumption': '10W'
            }
        }
        
        logger.info("è¾¹ç¼˜è®¡ç®—éƒ¨ç½²é…ç½®å®Œæˆ")
        return configs
        
    def get_performance_report(self) -> Dict:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        total_time = time.time() - self.metrics['start_time']
        return {
            'processed_count': self.metrics['processed_count'],
            'decision_count': self.metrics['decision_count'],
            'avg_latency_ms': self.metrics['avg_latency'] * 1000,
            'throughput_per_second': self.metrics['processed_count'] / max(1, total_time),
            'error_rate': self.metrics['error_count'] / max(1, self.metrics['processed_count']),
            'total_runtime': total_time
        }

def demo_realtime_engine():
    """æ¼”ç¤ºå®æ—¶å†³ç­–å¼•æ“"""
    print("ğŸš€ å®æ—¶å†³ç­–å¼•æ“æ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºå¼•æ“
    engine = SimpleRealtimeEngine()
    
    # è®¾ç½®å›è°ƒå‡½æ•°
    high_priority_decisions = []
    
    def priority_callback(decision: DecisionResult):
        if decision.confidence > 0.85:
            high_priority_decisions.append(decision)
            print(f"âš¡ é«˜ä¼˜å…ˆçº§å†³ç­–: {decision.action} | ç½®ä¿¡åº¦: {decision.confidence:.2f}")
    
    engine.add_decision_callback(priority_callback)
    
    # è¿è¡Œå®æ—¶å¤„ç†
    print("å¯åŠ¨å®æ—¶æ•°æ®æµå¤„ç†...")
    start_time = time.time()
    
    engine.process_live_data(duration_seconds=30)
    
    total_time = time.time() - start_time
    
    # è·å–æ€§èƒ½æŠ¥å‘Š
    report = engine.get_performance_report()
    
    print(f"\nğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ:")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"æ€»è¿è¡Œæ—¶é—´: {total_time:.1f} ç§’")
    print(f"å¤„ç†æ•°æ®é‡: {report['processed_count']} æ¡")
    print(f"å†³ç­–æ•°é‡: {report['decision_count']} ä¸ª")
    print(f"å¹³å‡å»¶è¿Ÿ: {report['avg_latency_ms']:.1f} ms")
    print(f"ååé‡: {report['throughput_per_second']:.1f} æ¡/ç§’")
    print(f"é”™è¯¯ç‡: {report['error_rate']:.2%}")
    print(f"é«˜ä¼˜å…ˆçº§å†³ç­–: {len(high_priority_decisions)} ä¸ª")
    
    # è¾¹ç¼˜è®¡ç®—éƒ¨ç½²æ¼”ç¤º
    print(f"\nğŸ”§ è¾¹ç¼˜è®¡ç®—éƒ¨ç½²æ¼”ç¤º:")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    edge_configs = engine.edge_computing_demo()
    
    for platform, config in edge_configs.items():
        print(f"ğŸ“± {platform}:")
        print(f"   æ¨ç†å»¶è¿Ÿ: {config['inference_latency']}")
        print(f"   å†…å­˜éœ€æ±‚: {config['memory_requirement']}")
        print(f"   æ¨¡å‹å¤§å°: {config['model_size']}")
        print(f"   åŠŸè€—: {config['power_consumption']}")
    
    # éªŒè¯å®æ—¶æ€§èƒ½æŒ‡æ ‡
    print(f"\nâœ… å®æ—¶æ€§èƒ½éªŒè¯:")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    latency_ok = report['avg_latency_ms'] < 100
    throughput_ok = report['throughput_per_second'] > 1
    error_rate_ok = report['error_rate'] < 0.01
    
    print(f"å»¶è¿Ÿè¦æ±‚ (<100ms): {'âœ…' if latency_ok else 'âŒ'} {report['avg_latency_ms']:.1f}ms")
    print(f"ååé‡è¦æ±‚ (>1æ¡/ç§’): {'âœ…' if throughput_ok else 'âŒ'} {report['throughput_per_second']:.1f}æ¡/ç§’")
    print(f"é”™è¯¯ç‡è¦æ±‚ (<1%): {'âœ…' if error_rate_ok else 'âŒ'} {report['error_rate']:.2%}")
    
    # è®¡ç®—ç»¼åˆè¯„åˆ†
    performance_score = min(100, (100 - report['avg_latency_ms']) + report['throughput_per_second'] * 10)
    reliability_score = (1 - report['error_rate']) * 100
    edge_score = 95  # è¾¹ç¼˜éƒ¨ç½²æ”¯æŒè¯„åˆ†
    
    overall_score = (performance_score + reliability_score + edge_score) / 3
    
    print(f"\nğŸ† ç»¼åˆè¯„ä¼°:")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"æ€§èƒ½è¯„åˆ†: {performance_score:.1f}/100")
    print(f"å¯é æ€§è¯„åˆ†: {reliability_score:.1f}/100")
    print(f"è¾¹ç¼˜éƒ¨ç½²è¯„åˆ†: {edge_score:.1f}/100")
    print(f"ç»¼åˆè¯„åˆ†: {overall_score:.1f}/100")
    
    if overall_score >= 90:
        grade = "ğŸ¥‡ ä¼˜ç§€"
    elif overall_score >= 80:
        grade = "ğŸ¥ˆ è‰¯å¥½"
    elif overall_score >= 70:
        grade = "ğŸ¥‰ åˆæ ¼"
    else:
        grade = "âŒ éœ€è¦æ”¹è¿›"
    
    print(f"ç­‰çº§è¯„å®š: {grade}")
    
    # åº”ç”¨åœºæ™¯åˆ†æ
    print(f"\nğŸ’¼ åº”ç”¨åœºæ™¯:")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"âœ… æ™ºèƒ½åˆ¶é€ å®æ—¶ç›‘æ§")
    print(f"âœ… ç”Ÿäº§çº¿è´¨é‡æ§åˆ¶")
    print(f"âœ… è®¾å¤‡é¢„æµ‹æ€§ç»´æŠ¤")
    print(f"âœ… ä¾›åº”é“¾å®æ—¶ä¼˜åŒ–")
    print(f"âœ… è¾¹ç¼˜è®¡ç®—éƒ¨ç½²")
    
    print(f"\nğŸ“ˆ æŠ€æœ¯ä¼˜åŠ¿:")
    print(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    print(f"â€¢ æ¯«ç§’çº§å“åº”å»¶è¿Ÿ")
    print(f"â€¢ å¢é‡æ±‚è§£æœºåˆ¶")
    print(f"â€¢ å¤šå¹³å°è¾¹ç¼˜éƒ¨ç½²")
    print(f"â€¢ è‡ªé€‚åº”å†³ç­–ç­–ç•¥")
    print(f"â€¢ é«˜å¯é æ€§ä¿éšœ")
    
    return {
        'performance': report,
        'edge_configs': edge_configs,
        'overall_score': overall_score,
        'high_priority_decisions': len(high_priority_decisions)
    }

if __name__ == '__main__':
    # è¿è¡Œæ¼”ç¤º
    results = demo_realtime_engine()
    
    # ä¿å­˜ç»“æœ
    with open('output/realtime_demo_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… å®æ—¶å†³ç­–å¼•æ“æ¼”ç¤ºå®Œæˆï¼")
    print(f"ç»“æœå·²ä¿å­˜åˆ°: output/realtime_demo_results.json") 